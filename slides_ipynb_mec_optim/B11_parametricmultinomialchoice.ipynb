{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Block 11: Multinomial choice II. Parametric inversion</center>\n",
    "### <center>Alfred Galichon (NYU)</center>\n",
    "## <center>`math+econ+code' masterclass on matching models, optimal transport and applications</center>\n",
    "<center>© 2018-2019 by Alfred Galichon. Support from NSF grant DMS-1716489 is acknowledged. James Nesbit contributed.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* Savage, L. (1951). The theory of statistical decision. JASA.\n",
    "* Bonnet, Fougère, Galichon, Poulhès (2019). Minimax estimation of hedonic models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric estimation\n",
    "\n",
    "Assume the utilities are parameterized as follows: $U = \\Phi \\beta$ where $\\beta\\in\\mathbb{R}^{p}$ is a parameter, and $\\Phi$ is a $\\left\\vert \\mathcal{Y}\\right\\vert \\times p$ matrix.\n",
    "\n",
    "The log-likelihood function is given by\n",
    "\n",
    "\\begin{align*}\n",
    "l\\left(  \\beta\\right)  =N\\sum_{y}\\hat{s}_{y}\\log\\sigma_{y}\\left(\\Phi \\beta\\right)\n",
    "\\end{align*}\n",
    "\n",
    "A common estimation method of $\\beta$ is by maximum likelihood%\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\beta}l\\left(  \\beta\\right)  .\n",
    "\\end{align*}\n",
    "\n",
    "MLE is statistically efficient; the problem is that the problem is not guaranteed to be convex, so there may be computational difficulties (e.g. local optima)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE, logit case\n",
    "\n",
    "In the logit case,\n",
    "\n",
    "\\begin{align*}\n",
    "l\\left(  \\beta\\right)  =N\\left\\{  \\hat{s}^{\\intercal}\\Phi\\beta-\\log\\sum_{y}\\exp\\left(  \\Phi\\beta\\right)  _{y}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "so that the max-likehood amounts to\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\beta}\\left\\{  \\hat{s}^{\\intercal} \\Phi \\beta-G\\left( \\Phi \\beta\\right)\n",
    "_{y}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "whose value is the Legendre-Fenchel transform of $\\beta\\rightarrow G\\left( \\Phi \\beta\\right)$ evaluated at $\\Phi ^{^{\\intercal}}\\hat{s}$.\n",
    "\n",
    "Note that the vector $\\Phi^{^{\\intercal}}\\hat{s}$ is the vector of empirical moments, which is a sufficient statistics in the logit model.\n",
    "\n",
    "As a result, in the logit case, the MLE is a convex optimization problem, and it is therefore both statistically efficient and computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moment estimation\n",
    "\n",
    "The previous remark will inspire an alternative procedure based on the moments statistics $\\Phi^{^{\\intercal}}\\hat{s}$.\n",
    "\n",
    "The social welfare is given in general by $W\\left(  \\beta\\right) =G\\left(  \\Phi\\beta\\right)  $. One has $\\partial_{\\beta^{i}}W\\left(\\beta\\right)  =\\sum_{y}\\sigma_{y}\\left(  \\Phi\\beta\\right)  \\Phi_{yi}$, that is \n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla W\\left(  \\beta\\right)  = \\Phi^{\\intercal}\\sigma\\left(  \\Phi\\beta\\right)  ,\n",
    "\\end{align*}\n",
    "\n",
    "which is the vector of predicted moments.\n",
    "\n",
    "Therefore the program\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\beta}\\left\\{  \\hat{s}^{\\intercal}\\Phi\\beta-G\\left(  \\Phi\\beta\\right)\n",
    "_{y}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "picks up the parameter $\\beta$ which matches the empirical moments $X^{^{\\intercal}}\\hat{s}$ with the predicted ones $\\nabla W\\left(\\beta\\right)  $. This procedure is not statistically efficient, but is computationally efficient becauses it arises from a convex optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed temperature MLE\n",
    "\n",
    "Back to the logit case. Recall we have\n",
    "\n",
    "\\begin{align*}\n",
    "l\\left(  \\beta\\right)  =N\\left\\{  \\hat{s}^{\\intercal}\\Phi\\beta-\\log\\sum_{y} \\exp\\left(  \\Phi\\beta\\right)  _{y}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "Assume that we restrict ourselves to $\\beta^{\\top}z>0$. Then we can write $\\beta=\\theta/T$ where $T=1/\\beta^{\\top}z$ and $\\theta=\\beta T$. Call $\\Theta=\\left\\{  \\theta\\in\\mathbb{R}^{p},\\theta^{\\top}z=1\\right\\}  $, so that $\\beta=\\theta/T$ where $\\theta\\in\\Theta$ and $T>0$. We have\n",
    "\n",
    "\\begin{align*}\n",
    "l\\left(  \\theta,T\\right)  =\\frac{N}{T}\\left\\{  \\hat{s}^{\\intercal}\n",
    "\\Phi\\theta-T\\log\\sum_{y}\\exp\\left(  \\frac{\\left(  \\Phi\\theta\\right)  _{y}}{T}\\right)  \\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "and we define the *fixed temperature maximum likelihood estimator* by\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  T\\right)  =\\arg\\max_{\\theta}l\\left(  \\theta,T\\right)\n",
    "\\end{align*}\n",
    "\n",
    " Note that $\\theta\\left(  T\\right)  =\\arg\\max_{\\theta\\in\\Theta}Tl\\left(\\theta,T\\right)$ where\n",
    "\n",
    "\\begin{align*}\n",
    "Tl\\left(  \\theta,T\\right)  =N\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta-T\\log\\sum _{y}\\exp\\left(  \\frac{\\left(  \\Phi\\theta\\right)  _{y}}{T}\\right)  \\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "and we note that $Tl\\left(  \\theta,T\\right)  \\rightarrow N\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta-\\max_{y\\in\\mathcal{Y}}\\left\\{  \\left(  \\Phi\\theta\\right)_{y}\\right\\}  \\right\\}  $ as $T\\rightarrow0$.\n",
    "\n",
    "We have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{Tl\\left(  \\theta,T\\right)  }{N}=\\hat{s}^{\\intercal}\\Phi\\theta-T\\log\\sum_{y}\\exp\\left(  \\frac{\\left(  \\Phi\\theta\\right)  _{y}}{T}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Let $\\theta\\left(  0\\right)  =\\lim_{T\\rightarrow0}\\theta\\left(T\\right)  $. Calling $m\\left(  \\theta\\right)  =\\max_{y\\in\\mathcal{Y}}\\left\\{\\left(  \\Phi\\theta\\right)  _{y}\\right\\}  $, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  0\\right)  \\in\\arg\\max_{\\theta}\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta-m\\left(  \\theta\\right)  \\right\\},\n",
    "\\end{align*}\n",
    "\n",
    "or\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  0\\right)  \\in\\arg\\min_{\\theta}\\left\\{  m\\left(  \\theta\\right)-\\hat{s}^{\\intercal}\\Phi\\theta\\right\\},\n",
    "\\end{align*}\n",
    "\n",
    "Calling $m\\left(  \\theta\\right)  =\\max_{y\\in\\mathcal{Y}}\\left\\{  \\left(\\Phi\\theta\\right)  _{y}\\right\\}  $, one has \n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  T\\right)  \\in\\arg\\max\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta-m\\left(  \\theta\\right)  -T\\log\\sum_{y}\\exp\\left(  \\frac{\\left(\\Phi\\theta\\right)  _{y}-m\\left(  \\theta\\right)  }{T}\\right)  \\right\\}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimax-regret estimation\n",
    "\n",
    "Note that\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  0\\right)  \\in\\arg\\max\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta\n",
    "-m\\left(  \\theta\\right)  \\right\\}  .\n",
    "\\end{align*}\n",
    "\n",
    "Define $R_{i}\\left(  \\theta,y\\right)  =\\left(  \\Phi\\theta\\right)_{y}-\\left(  \\Phi\\theta\\right)  _{y_{i}}$ the regret associated with observation $i$ with respect to $y$. This is equal to the difference between the payoff given by $y$ and the payoff obtained under observation $i$, denoting $y_{i}$ the action taken in observation $i$. The max-regret associated with observation $i$ is therefore\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{y\\in\\mathcal{Y}}R_{i}\\left(  \\theta,y\\right)  =\\max_{y\\in\\mathcal{Y}}\\left\\{  \\left(  \\Phi\\theta\\right)_{y}-\\left(  \\Phi\\theta\\right)_{y_{i}}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "and the max-regret associated with the sample is $\\frac{1}{N}\\sum\\max_{y\\in\\mathcal{Y}}\\left\\{  R_{i}\\left(  \\theta,y\\right)  \\right\\}  $, that is $\\max_{y\\in\\mathcal{Y}}\\left\\{  \\left(  \\Phi\\theta\\right)  _{y}\\right\\} - \\hat{s}^{\\intercal}X\\theta$.\n",
    "\n",
    "The minimax regret estimator\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\theta}^{MMR}=\\min_{\\theta}\\left\\{  m\\left(  \\theta\\right)  -\\hat\n",
    "{s}^{\\intercal}\\Phi\\theta\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "which has a linear programming fomulation\n",
    "\n",
    "\\begin{align*}\n",
    "&  \\min_{m,\\theta}m-\\hat{s}^{\\intercal}\\Phi\\theta\\\\\n",
    "s.t.~ &  m-\\left(  \\Phi\\theta\\right)  _{y}\\geq\\forall y\\in\\mathcal{Y}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-identification\n",
    "\n",
    "Note that the set of $\\theta$ that enter the solution to the problem above is not unique, but is a convex set. Denoting $V$ the value of program, we can look for bounds of $\\theta^{\\intercal}d$ for a chosen direction $d$ by\n",
    "\n",
    "\\begin{align*}\n",
    "& \\min_{\\theta,m}/\\max_{\\theta,m}   \\theta^{\\intercal}d\\\\\n",
    "s.t.~  &  m-\\hat{s}^{\\intercal}X\\theta=V\\\\\n",
    "&  m\\geq\\left(  \\Phi\\theta\\right)_{y}, \\quad \\forall y\\in\\mathcal{Y}%\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Back to the travel mode example of Greene and Hensher (1997). For each individual $i$, we have access to: $y$=travel mode (air, train, bus, car); $T_{iy}$=time taken (observed); $C_{iy}$=generalized cost for passenger (observed); $I_{i}$=income; $y_{j}$=travel mode actually chosen.\n",
    "* Load the data using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'numDeriv' was built under R version 3.5.2\"Loading required package: slam\n",
      "Warning message:\n",
      "\"package 'slam' was built under R version 3.5.2\""
     ]
    }
   ],
   "source": [
    "library(Matrix)\n",
    "library(numDeriv)\n",
    "library(gurobi)\n",
    "\n",
    "thePath = getwd()\n",
    "travelmodedataset = as.matrix(read.csv(paste0(thePath, \"/../data_mec_optim/demand_travelmode/travelmodedata.csv\"), sep = \",\", \n",
    "    header = TRUE))  # loads the data\n",
    "# Convert strings to categorical variables\n",
    "convertmode = Vectorize(function(inputtxt) {\n",
    "    if (inputtxt == \"air\") {\n",
    "        return(1)\n",
    "    }\n",
    "    if (inputtxt == \"train\") {\n",
    "        return(2)\n",
    "    }\n",
    "    if (inputtxt == \"bus\") {\n",
    "        return(3)\n",
    "    }\n",
    "    if (inputtxt == \"car\") {\n",
    "        return(4)\n",
    "    }\n",
    "})\n",
    "\n",
    "convertchoice = function(x) (ifelse(x==\"no\",0,1))\n",
    "\n",
    "travelmodedataset[,2] = convertmode(travelmodedataset[,2])\n",
    "travelmodedataset[,3] = convertchoice(travelmodedataset[,3])\n",
    "nobs = dim(travelmodedataset)[1]\n",
    "nchoices = 4\n",
    "ninds = nobs / nchoices\n",
    "ncols =  dim(travelmodedataset)[2]\n",
    "travelmodedataset = array(as.numeric(travelmodedataset),dim = c(4,ninds,ncols))\n",
    "muhat_i_y = t(travelmodedataset[,,3])\n",
    "muhat_iy = c(muhat_i_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our model is:\n",
    "    + $u_{iy} = U_{y}-T_{iy}(a+bI_{i})-c C_{iy}$\n",
    "* We need to take an additive normalization, so let's set $U_1 = 0$. \n",
    "* We need to take a scale normalization, so let's set $c = 1 $ so that utilities are labelled in monetary units.\n",
    "* Setup the matrix $\\Phi$ using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_iy_k=cbind( kronecker(sparseMatrix(i = c(2,3,4),j=c(1,2,3)),matrix(1,ninds,1) ), # fixed effect with normalization U_1 = 0\n",
    "              - c(t(travelmodedataset[,,6])), # time\n",
    "              - c(t(travelmodedataset[,,6]*c(travelmodedataset[,,8]))), # time*incime\n",
    "              - c(t(travelmodedataset[,,7]) ) # cost\n",
    ")\n",
    "nbK = dim(Phi_iy_k)[2]\n",
    "\n",
    "mean_k = apply(Phi_iy_k,FUN = mean , MARGIN =  2)\n",
    "std_k = apply(Phi_iy_k,FUN = sd , MARGIN =  2)\n",
    "\n",
    "Phi_iy_k = (Phi_iy_k - matrix(mean_k,nobs,nbK, byrow = T)) / matrix(std_k,nobs,nbK, byrow = T)\n",
    "\n",
    "theta0 = rep(0,nbK)\n",
    "sigma = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, define the log-likelihood function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logLikelihood = function (theta ) {\n",
    "  nbk = length(theta)\n",
    "    \n",
    "  Xtheta = Phi_iy_k %*% theta / sigma\n",
    "    \n",
    "  Xthetamat_iy= matrix(Xtheta,ninds,nchoices)\n",
    "    \n",
    "  max_i = apply(X=Xthetamat_iy,FUN = max,MARGIN = 1)\n",
    "    \n",
    "  expPhi_iy = exp(Xthetamat_iy - matrix(max_i,ninds,nchoices))\n",
    "  d_i = apply(X=expPhi_iy , FUN=sum,MARGIN = 1 )\n",
    "  n_i_k = apply(X= array (Phi_iy_k*c(expPhi_iy),dim = c(ninds,nchoices,nbK) ), FUN=sum,MARGIN = c(1,3) )\n",
    "  thegrad = c(as.matrix(matrix(muhat_iy,1,nchoices*ninds) %*% Phi_iy_k))- apply( X = n_i_k / d_i, FUN = sum, MARGIN=2)\n",
    "  res= sum(Xtheta*muhat_iy)  - sum(max_i) - sigma * sum(log(d_i ))\n",
    "  \n",
    "  thegrad  = - thegrad\n",
    "  res = - res\n",
    "  \n",
    "  attr(res,'gradient') = thegrad\n",
    "  return(res)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The maximization of the log-likelihood is done using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "2.2822646287307"
      ],
      "text/latex": [
       "2.2822646287307"
      ],
      "text/markdown": [
       "2.2822646287307"
      ],
      "text/plain": [
       "[1] 2.282265"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1.623157093739</li>\n",
       "\t<li>0.836018871756733</li>\n",
       "\t<li>1.16074845889259</li>\n",
       "\t<li>0.942298349323567</li>\n",
       "\t<li>1.19583108408635</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1.623157093739\n",
       "\\item 0.836018871756733\n",
       "\\item 1.16074845889259\n",
       "\\item 0.942298349323567\n",
       "\\item 1.19583108408635\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1.623157093739\n",
       "2. 0.836018871756733\n",
       "3. 1.16074845889259\n",
       "4. 0.942298349323567\n",
       "5. 1.19583108408635\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1.6231571 0.8360189 1.1607485 0.9422983 1.1958311"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outcome_mle = nlm(f = logLikelihood, p = theta0 , gradtol=1E-8)\n",
    "temp_mle = 1 / outcome_mle$estimate[nbK]\n",
    "theta_mle = outcome_mle$estimate[-nbK] * temp_mle\n",
    "temp_mle\n",
    "theta_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We now compute the minimax-regret estimator using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimize a model with 841 rows, 216 columns and 5881 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [3e-03, 5e+00]\n",
      "  Objective range  [1e+00, 5e+01]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Presolve removed 211 rows and 1 columns\n",
      "Presolve time: 0.03s\n",
      "Presolved: 630 rows, 215 columns, 2520 nonzeros\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0      handle free variables                          0s\n",
      "     359   -5.2644494e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 359 iterations and 0.14 seconds\n",
      "Optimal objective -5.264449397e+01\n"
     ]
    }
   ],
   "source": [
    "obj = c(c(as.matrix(matrix(muhat_iy,1,nchoices*ninds) %*% Phi_iy_k)),-rep(1,ninds))\n",
    "lengthobj = length(obj)\n",
    "cstMat = cbind( -Phi_iy_k, kronecker(matrix(1,nchoices,1),sparseMatrix(i = 1:ninds , j = 1:ninds,x = 1 ))  )\n",
    "cstMat = rbind(cstMat,c(rep(0,nbK-1),1,rep(0,ninds)))\n",
    "nbCstr = dim(cstMat)[1]\n",
    "result = gurobi(list(A = cstMat, obj = obj, modelsense = \"max\", rhs = c(rep(0,nbCstr-1),1), sense =  c(rep(\">\",nbCstr-1),\"=\"),lb=-Inf))\n",
    "theta_lp = result$x[1:nbK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare the MLE and the minmax-regret estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1.6231571 0.8360189 1.1607485 0.9422983 1.1958311\n",
      "[1] -0.122396477 -0.249558217 -0.320979871 -0.487460448  0.005989811\n"
     ]
    }
   ],
   "source": [
    "print(theta_mle)\n",
    "print(theta_lp[-nbK])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the fixed temperature MLE for intermediate temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "indMax=100\n",
    "tempMax=temp_mle\n",
    "outcomemat = matrix(0,indMax+1,nbK-1)\n",
    "for (k in 2:(indMax+1))\n",
    "{\n",
    "  thetemp = tempMax * (k-1)/indMax\n",
    "  logLikelihoodFixedTemp = function(subsetoftheta )\n",
    "  {\n",
    "    theres = logLikelihood(c(subsetoftheta,1/thetemp))\n",
    "    attr(theres,'gradient') = attr(theres,'gradient')[-nbK]\n",
    "    #print(c(theres))\n",
    "    return(theres)\n",
    "  }\n",
    "  outcomeFixedTemp = nlm(f = logLikelihoodFixedTemp, p = theta0[-nbK] , gradtol=1E-8)\n",
    "  outcomemat[k,] = outcomeFixedTemp$estimate * thetemp\n",
    "}\n",
    "outcomemat[1,] = theta_lp[-nbK]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The intermediate estimators are\"\n",
      "               [,1]         [,2]        [,3]        [,4]         [,5]\n",
      "  [1,] -0.122396477 -0.249558217 -0.32097987 -0.48746045  0.005989811\n",
      "  [2,] -0.126219651 -0.252231708 -0.32555473 -0.47348949 -0.011557708\n",
      "  [3,] -0.124567914 -0.250040395 -0.32603382 -0.46293955 -0.019488629\n",
      "  [4,] -0.118157348 -0.246825009 -0.32296427 -0.45360939 -0.020710183\n",
      "  [5,] -0.109873666 -0.244497524 -0.31906523 -0.44486755 -0.019691446\n",
      "  [6,] -0.100127575 -0.242036380 -0.31432657 -0.43620704 -0.017057771\n",
      "  [7,] -0.089148581 -0.238854106 -0.30860235 -0.42726313 -0.013128173\n",
      "  [8,] -0.077141838 -0.234766721 -0.30184231 -0.41783170 -0.008120978\n",
      "  [9,] -0.064281349 -0.229781964 -0.29406842 -0.40783608 -0.002177913\n",
      " [10,] -0.050707711 -0.223979877 -0.28534434 -0.39727803  0.004599193\n",
      " [11,] -0.036533091 -0.217459282 -0.27575464 -0.38619990  0.012124610\n",
      " [12,] -0.021847487 -0.210316146 -0.26539112 -0.37466038  0.020319378\n",
      " [13,] -0.006723794 -0.202635789 -0.25434403 -0.36272047  0.029109450\n",
      " [14,]  0.008778398 -0.194491104 -0.24269719 -0.35043649  0.038426321\n",
      " [15,]  0.024609956 -0.185943265 -0.23052575 -0.33785747  0.048207875\n",
      " [16,]  0.040730033 -0.177043313 -0.21789564 -0.32502470  0.058398745\n",
      " [17,]  0.057104374 -0.167833912 -0.20486400 -0.31197251  0.068950191\n",
      " [18,]  0.073704092 -0.158350848 -0.19147997 -0.29872911  0.079819539\n",
      " [19,]  0.090504486 -0.148624503 -0.17778575 -0.28531802  0.090969738\n",
      " [20,]  0.107484395 -0.138680773 -0.16381758 -0.27175870  0.102368531\n",
      " [21,]  0.124625521 -0.128541953 -0.14960658 -0.25806751  0.113987889\n",
      " [22,]  0.141911940 -0.118227378 -0.13517957 -0.24425826  0.125803440\n",
      " [23,]  0.159329717 -0.107753941 -0.12055976 -0.23034278  0.137793940\n",
      " [24,]  0.176866608 -0.097136461 -0.10576728 -0.21633123  0.149940858\n",
      " [25,]  0.194511798 -0.086388018 -0.09081963 -0.20223242  0.162227962\n",
      " [26,]  0.212255698 -0.075520211 -0.07573213 -0.18805407  0.174641023\n",
      " [27,]  0.230089780 -0.064543355 -0.06051820 -0.17380298  0.187167549\n",
      " [28,]  0.248006436 -0.053466656 -0.04518966 -0.15948516  0.199796543\n",
      " [29,]  0.265998862 -0.042298351 -0.02975691 -0.14510597  0.212518296\n",
      " [30,]  0.284060954 -0.031045836 -0.01422919 -0.13067019  0.225324217\n",
      " [31,]  0.302187223 -0.019715762  0.00138529 -0.11618211  0.238206686\n",
      " [32,]  0.320372721 -0.008314127  0.01707924 -0.10164561  0.251158931\n",
      " [33,]  0.338612975  0.003153644  0.03284612 -0.08706418  0.264174931\n",
      " [34,]  0.356903954  0.014682657  0.04868011 -0.07244093  0.277249248\n",
      " [35,]  0.375241961  0.026268429  0.06457591 -0.05777882  0.290377097\n",
      " [36,]  0.393623685  0.037906939  0.08052883 -0.04308042  0.303554160\n",
      " [37,]  0.412046081  0.049594494  0.09653460 -0.02834815  0.316776537\n",
      " [38,]  0.430506382  0.061327731  0.11258934 -0.01358417  0.330040730\n",
      " [39,]  0.449002055  0.073103580  0.12868956  0.00120949  0.343343580\n",
      " [40,]  0.467530787  0.084919231  0.14483207  0.01603100  0.356682228\n",
      " [41,]  0.486090453  0.096772106  0.16101399  0.03087866  0.370054085\n",
      " [42,]  0.504679104  0.108659841  0.17723266  0.04575091  0.383456801\n",
      " [43,]  0.523294947  0.120580263  0.19348568  0.06064630  0.396888237\n",
      " [44,]  0.541936331  0.132531368  0.20977083  0.07556351  0.410346444\n",
      " [45,]  0.560601734  0.144511310  0.22608609  0.09050130  0.423829642\n",
      " [46,]  0.579289748  0.156518387  0.24242958  0.10545853  0.437336203\n",
      " [47,]  0.597999073  0.168551022  0.25879960  0.12043412  0.450864638\n",
      " [48,]  0.616728503  0.180607755  0.27519457  0.13542711  0.464413577\n",
      " [49,]  0.635476921  0.192687236  0.29161302  0.15043656  0.477981763\n",
      " [50,]  0.654243290  0.204788209  0.30805361  0.16546162  0.491568038\n",
      " [51,]  0.673026645  0.216909507  0.32451508  0.18050149  0.505171334\n",
      " [52,]  0.691826087  0.229050047  0.34099629  0.19555543  0.518790665\n",
      " [53,]  0.710640780  0.241208816  0.35749616  0.21062273  0.532425119\n",
      " [54,]  0.729469943  0.253384874  0.37401368  0.22570276  0.546073853\n",
      " [55,]  0.748312848  0.265577340  0.39054792  0.24079488  0.559736082\n",
      " [56,]  0.767168812  0.277785393  0.40709803  0.25589854  0.573411080\n",
      " [57,]  0.786037198  0.290008264  0.42366319  0.27101319  0.587098171\n",
      " [58,]  0.804917409  0.302245234  0.44024265  0.28613833  0.600796727\n",
      " [59,]  0.823808883  0.314495627  0.45683570  0.30127348  0.614506159\n",
      " [60,]  0.842711096  0.326758811  0.47344168  0.31641820  0.628225922\n",
      " [61,]  0.861623552  0.339034191  0.49005997  0.33157205  0.641955504\n",
      " [62,]  0.880545788  0.351321210  0.50669000  0.34673466  0.655694428\n",
      " [63,]  0.899477367  0.363619341  0.52333122  0.36190563  0.669442245\n",
      " [64,]  0.918417875  0.375928091  0.53998311  0.37708461  0.683198536\n",
      " [65,]  0.937366926  0.388246993  0.55664520  0.39227128  0.696962909\n",
      " [66,]  0.956324153  0.400575608  0.57331703  0.40746532  0.710734994\n",
      " [67,]  0.975289209  0.412913520  0.58999817  0.42266641  0.724514443\n",
      " [68,]  0.994261769  0.425260339  0.60668823  0.43787429  0.738300930\n",
      " [69,]  1.013241523  0.437615694  0.62338682  0.45308868  0.752094147\n",
      " [70,]  1.032228179  0.449979235  0.64009359  0.46830933  0.765893806\n",
      " [71,]  1.051221460  0.462350630  0.65680819  0.48353600  0.779699631\n",
      " [72,]  1.070221103  0.474729565  0.67353031  0.49876844  0.793511366\n",
      " [73,]  1.089226859  0.487115743  0.69025964  0.51400646  0.807328766\n",
      " [74,]  1.108238493  0.499508881  0.70699589  0.52924983  0.821151601\n",
      " [75,]  1.127255779  0.511908711  0.72373879  0.54449837  0.834979652\n",
      " [76,]  1.146278505  0.524314979  0.74048809  0.55975187  0.848812714\n",
      " [77,]  1.165306467  0.536727442  0.75724354  0.57501017  0.862650589\n",
      " [78,]  1.184339474  0.549145871  0.77400490  0.59027310  0.876493091\n",
      " [79,]  1.203377341  0.561570047  0.79077195  0.60554048  0.890340046\n",
      " [80,]  1.222419894  0.573999760  0.80754449  0.62081218  0.904191285\n",
      " [81,]  1.241466966  0.586434813  0.82432230  0.63608802  0.918046648\n",
      " [82,]  1.260518398  0.598875017  0.84110521  0.65136789  0.931905986\n",
      " [83,]  1.279574039  0.611320190  0.85789303  0.66665163  0.945769153\n",
      " [84,]  1.298633744  0.623770161  0.87468559  0.68193912  0.959636012\n",
      " [85,]  1.317697375  0.636224765  0.89148271  0.69723024  0.973506434\n",
      " [86,]  1.336764799  0.648683846  0.90828425  0.71252487  0.987380293\n",
      " [87,]  1.355835892  0.661147252  0.92509005  0.72782290  1.001257471\n",
      " [88,]  1.374910531  0.673614841  0.94189997  0.74312421  1.015137855\n",
      " [89,]  1.393988602  0.686086475  0.95871387  0.75842871  1.029021335\n",
      " [90,]  1.413069994  0.698562023  0.97553161  0.77373628  1.042907810\n",
      " [91,]  1.432154602  0.711041359  0.99235309  0.78904685  1.056797179\n",
      " [92,]  1.451242322  0.723524363  1.00917816  0.80436031  1.070689349\n",
      " [93,]  1.470333059  0.736010919  1.02600673  0.81967658  1.084584230\n",
      " [94,]  1.489426719  0.748500917  1.04283867  0.83499557  1.098481733\n",
      " [95,]  1.508523213  0.760994250  1.05967388  0.85031721  1.112381778\n",
      " [96,]  1.527622454  0.773490816  1.07651226  0.86564141  1.126284283\n",
      " [97,]  1.546724361  0.785990519  1.09335372  0.88096810  1.140189174\n",
      " [98,]  1.565828853  0.798493263  1.11019815  0.89629720  1.154096376\n",
      " [99,]  1.584935855  0.810998959  1.12704548  0.91162865  1.168005820\n",
      "[100,]  1.604045294  0.823507520  1.14389561  0.92696239  1.181917439\n",
      "[101,]  1.623157099  0.836018862  1.16074846  0.94229834  1.195831167\n"
     ]
    }
   ],
   "source": [
    "print('The intermediate estimators are')\n",
    "print(outcomemat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Compute the minimax estimator of the simulated logit mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbB=100\n",
    "thetemp = 1\n",
    "epsilon_biy =  array(digamma(1) - log(-log(runif(nbB*ninds*nchoices))), dim=c(nbB,ninds,nchoices))\n",
    "muhat_biy = rep(muhat_i_y,each=nbB)\n",
    "newobj = c(c(as.matrix(matrix(muhat_iy,1,nchoices*ninds) %*% Phi_iy_k)),-rep(1,ninds*nbB)/nbB  )\n",
    "newlengthobj = length(newobj)\n",
    "cstr1 = kronecker(-Phi_iy_k,matrix(1,nbB,1))\n",
    "newcstMat = cbind( kronecker(-Phi_iy_k,matrix(1,nbB,1)) , kronecker(matrix(1,nchoices,1),sparseMatrix(i = 1:(ninds*nbB) , j = 1:(ninds*nbB),x = 1 ))  )\n",
    "newnbCstr = dim(newcstMat)[1]\n",
    "newresult = gurobi(list(A = newcstMat, obj = newobj, modelsense = \"max\", rhs = c(epsilon_biy), sense =  \">\",lb=-Inf), params = list(OutputFlag = 0))\n",
    "\n",
    "newtheta_lp = newresult$x[1:nbK] / newresult$x[nbK] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare the MLE estimator in the logit model and the minimax regret estimator in the simulated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1.623157093739</li>\n",
       "\t<li>0.836018871756733</li>\n",
       "\t<li>1.16074845889259</li>\n",
       "\t<li>0.942298349323567</li>\n",
       "\t<li>1.19583108408635</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1.623157093739\n",
       "\\item 0.836018871756733\n",
       "\\item 1.16074845889259\n",
       "\\item 0.942298349323567\n",
       "\\item 1.19583108408635\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1.623157093739\n",
       "2. 0.836018871756733\n",
       "3. 1.16074845889259\n",
       "4. 0.942298349323567\n",
       "5. 1.19583108408635\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1.6231571 0.8360189 1.1607485 0.9422983 1.1958311"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1.55164545995868</li>\n",
       "\t<li>0.798438775343847</li>\n",
       "\t<li>1.10735915153293</li>\n",
       "\t<li>0.948709299942269</li>\n",
       "\t<li>1.06451164568853</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1.55164545995868\n",
       "\\item 0.798438775343847\n",
       "\\item 1.10735915153293\n",
       "\\item 0.948709299942269\n",
       "\\item 1.06451164568853\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1.55164545995868\n",
       "2. 0.798438775343847\n",
       "3. 1.10735915153293\n",
       "4. 0.948709299942269\n",
       "5. 1.06451164568853\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1.5516455 0.7984388 1.1073592 0.9487093 1.0645116"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "theta_mle\n",
    "newtheta_lp[-nbK]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us now try a Probit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1.6231571 0.8360189 1.1607485 0.9422983 1.1958311\n",
      "[1] 1.5944982 0.8109769 1.1194933 0.8838003 1.2304748\n"
     ]
    }
   ],
   "source": [
    "nbB=500\n",
    "thetemp = 1\n",
    "epsilon_biy =  array(rnorm(nbB*ninds*nchoices), dim=c(nbB,ninds,nchoices))\n",
    "muhat_biy = rep(muhat_i_y,each=nbB)\n",
    "newobj = c(c(as.matrix(matrix(muhat_iy,1,nchoices*ninds) %*% Phi_iy_k)),-rep(1,ninds*nbB)/nbB  )\n",
    "newlengthobj = length(newobj)\n",
    "cstr1 = kronecker(-Phi_iy_k,matrix(1,nbB,1))\n",
    "newcstMat = cbind( kronecker(-Phi_iy_k,matrix(1,nbB,1)) , kronecker(matrix(1,nchoices,1),sparseMatrix(i = 1:(ninds*nbB) , j = 1:(ninds*nbB),x = 1 ))  )\n",
    "newnbCstr = dim(newcstMat)[1]\n",
    "newresult = gurobi(list(A = newcstMat, obj = newobj, modelsense = \"max\", rhs = c(epsilon_biy), sense =  \">\",lb=-Inf), params = list(OutputFlag = 0))\n",
    "\n",
    "newtheta_lp_probit = newresult$x[1:nbK] / newresult$x[nbK] \n",
    "\n",
    "print(theta_mle)\n",
    "print(newtheta_lp_probit[-nbK])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
