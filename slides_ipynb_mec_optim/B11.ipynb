{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Block 11: Multinomial choice II. Parametric inversion</center>\n",
    "### <center>Alfred Galichon (NYU)</center>\n",
    "## <center>`math+econ+code' masterclass on matching models, optimal transport and applications</center>\n",
    "<center>© 2018-2019 by Alfred Galichon. Support from NSF grant DMS-1716489 is acknowledged. James Nesbit contributed.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* Savage, L. (1951). The theory of statistical decision. JASA.\n",
    "* Bonnet, Fougère, Galichon, Poulhès (2019). Minimax estimation of hedonic models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric estimation\n",
    "\n",
    "Assume the utilities are parameterized as follows: $U = \\Phi \\beta$ where $\\beta\\in\\mathbb{R}^{p}$ is a parameter, and $\\Phi$ is a $\\left\\vert \\mathcal{Y}\\right\\vert \\times p$ matrix.\n",
    "\n",
    "The log-likelihood function is given by\n",
    "\n",
    "\\begin{align*}\n",
    "l\\left(  \\beta\\right)  =N\\sum_{y}\\hat{s}_{y}\\log\\sigma_{y}\\left(\\Phi \\beta\\right)\n",
    "\\end{align*}\n",
    "\n",
    "A common estimation method of $\\beta$ is by maximum likelihood%\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\beta}l\\left(  \\beta\\right)  .\n",
    "\\end{align*}\n",
    "\n",
    "MLE is statistically efficient; the problem is that the problem is not guaranteed to be convex, so there may be computational difficulties (e.g. local optima)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE, logit case\n",
    "\n",
    "In the logit case,\n",
    "\n",
    "\\begin{align*}\n",
    "l\\left(  \\beta\\right)  =N\\left\\{  \\hat{s}^{\\intercal}\\Phi\\beta-\\log\\sum_{y}\\exp\\left(  \\Phi\\beta\\right)  _{y}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "so that the max-likehood amounts to\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\beta}\\left\\{  \\hat{s}^{\\intercal} \\Phi \\beta-G\\left( \\Phi \\beta\\right)\n",
    "_{y}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "whose value is the Legendre-Fenchel transform of $\\beta\\rightarrow G\\left( \\Phi \\beta\\right)$ evaluated at $\\Phi ^{^{\\intercal}}\\hat{s}$.\n",
    "\n",
    "Note that the vector $\\Phi^{^{\\intercal}}\\hat{s}$ is the vector of empirical moments, which is a sufficient statistics in the logit model.\n",
    "\n",
    "As a result, in the logit case, the MLE is a convex optimization problem, and it is therefore both statistically efficient and computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moment estimation\n",
    "\n",
    "The previous remark will inspire an alternative procedure based on the moments statistics $\\Phi^{^{\\intercal}}\\hat{s}$.\n",
    "\n",
    "The social welfare is given in general by $W\\left(  \\beta\\right) =G\\left(  \\Phi\\beta\\right)  $. One has $\\partial_{\\beta^{i}}W\\left(\\beta\\right)  =\\sum_{y}\\sigma_{y}\\left(  \\Phi\\beta\\right)  \\Phi_{yi}$, that is \n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla W\\left(  \\beta\\right)  = \\Phi^{\\intercal}\\sigma\\left(  \\Phi\\beta\\right)  ,\n",
    "\\end{align*}\n",
    "\n",
    "which is the vector of predicted moments.\n",
    "\n",
    "Therefore the program\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\beta}\\left\\{  \\hat{s}^{\\intercal}\\Phi\\beta-G\\left(  \\Phi\\beta\\right)\n",
    "_{y}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "picks up the parameter $\\beta$ which matches the empirical moments $X^{^{\\intercal}}\\hat{s}$ with the predicted ones $\\nabla W\\left(\\beta\\right)  $. This procedure is not statistically efficient, but is computationally efficient becauses it arises from a convex optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed temperature MLE\n",
    "\n",
    "Back to the logit case. Recall we have\n",
    "\n",
    "\\begin{align*}\n",
    "l\\left(  \\beta\\right)  =N\\left\\{  \\hat{s}^{\\intercal}\\Phi\\beta-\\log\\sum_{y} \\exp\\left(  \\Phi\\beta\\right)  _{y}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "Assume that we restrict ourselves to $\\beta^{\\top}z>0$. Then we can write $\\beta=\\theta/T$ where $T=1/\\beta^{\\top}z$ and $\\theta=\\beta T$. Call $\\Theta=\\left\\{  \\theta\\in\\mathbb{R}^{p},\\theta^{\\top}z=1\\right\\}  $, so that $\\beta=\\theta/T$ where $\\theta\\in\\Theta$ and $T>0$. We have\n",
    "\n",
    "\\begin{align*}\n",
    "l\\left(  \\theta,T\\right)  =\\frac{N}{T}\\left\\{  \\hat{s}^{\\intercal}\n",
    "\\Phi\\theta-T\\log\\sum_{y}\\exp\\left(  \\frac{\\left(  \\Phi\\theta\\right)  _{y}}{T}\\right)  \\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "and we define the *fixed temperature maximum likelihood estimator* by\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  T\\right)  =\\arg\\max_{\\theta}l\\left(  \\theta,T\\right)\n",
    "\\end{align*}\n",
    "\n",
    " Note that $\\theta\\left(  T\\right)  =\\arg\\max_{\\theta\\in\\Theta}Tl\\left(\\theta,T\\right)$ where\n",
    "\n",
    "\\begin{align*}\n",
    "Tl\\left(  \\theta,T\\right)  =N\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta-T\\log\\sum _{y}\\exp\\left(  \\frac{\\left(  \\Phi\\theta\\right)  _{y}}{T}\\right)  \\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "and we note that $Tl\\left(  \\theta,T\\right)  \\rightarrow N\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta-\\max_{y\\in\\mathcal{Y}}\\left\\{  \\left(  \\Phi\\theta\\right)_{y}\\right\\}  \\right\\}  $ as $T\\rightarrow0$.\n",
    "\n",
    "We have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{Tl\\left(  \\theta,T\\right)  }{N}=\\hat{s}^{\\intercal}\\Phi\\theta-T\\log\\sum_{y}\\exp\\left(  \\frac{\\left(  \\Phi\\theta\\right)  _{y}}{T}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Let $\\theta\\left(  0\\right)  =\\lim_{T\\rightarrow0}\\theta\\left(T\\right)  $. Calling $m\\left(  \\theta\\right)  =\\max_{y\\in\\mathcal{Y}}\\left\\{\\left(  \\Phi\\theta\\right)  _{y}\\right\\}  $, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  0\\right)  \\in\\arg\\max_{\\theta}\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta-m\\left(  \\theta\\right)  \\right\\},\n",
    "\\end{align*}\n",
    "\n",
    "or\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  0\\right)  \\in\\arg\\min_{\\theta}\\left\\{  m\\left(  \\theta\\right)-\\hat{s}^{\\intercal}\\Phi\\theta\\right\\},\n",
    "\\end{align*}\n",
    "\n",
    "Calling $m\\left(  \\theta\\right)  =\\max_{y\\in\\mathcal{Y}}\\left\\{  \\left(\\Phi\\theta\\right)  _{y}\\right\\}  $, one has \n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  T\\right)  \\in\\arg\\max\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta-m\\left(  \\theta\\right)  -T\\log\\sum_{y}\\exp\\left(  \\frac{\\left(\\Phi\\theta\\right)  _{y}-m\\left(  \\theta\\right)  }{T}\\right)  \\right\\}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimax-regret estimation\n",
    "\n",
    "Note that\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\\left(  0\\right)  \\in\\arg\\max\\left\\{  \\hat{s}^{\\intercal}\\Phi\\theta\n",
    "-m\\left(  \\theta\\right)  \\right\\}  .\n",
    "\\end{align*}\n",
    "\n",
    "Define $R_{i}\\left(  \\theta,y\\right)  =\\left(  \\Phi\\theta\\right)_{y}-\\left(  \\Phi\\theta\\right)  _{y_{i}}$ the regret associated with observation $i$ with respect to $y$. This is equal to the difference between the payoff given by $y$ and the payoff obtained under observation $i$, denoting $y_{i}$ the action taken in observation $i$. The max-regret associated with observation $i$ is therefore\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{y\\in\\mathcal{Y}}R_{i}\\left(  \\theta,y\\right)  =\\max_{y\\in\\mathcal{Y}}\\left\\{  \\left(  \\Phi\\theta\\right)_{y}-\\left(  \\Phi\\theta\\right)_{y_{i}}\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "and the max-regret associated with the sample is $\\frac{1}{N}\\sum\\max_{y\\in\\mathcal{Y}}\\left\\{  R_{i}\\left(  \\theta,y\\right)  \\right\\}  $, that is $\\max_{y\\in\\mathcal{Y}}\\left\\{  \\left(  \\Phi\\theta\\right)  _{y}\\right\\} - \\hat{s}^{\\intercal}X\\theta$.\n",
    "\n",
    "The minimax regret estimator\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\theta}^{MMR}=\\min_{\\theta}\\left\\{  m\\left(  \\theta\\right)  -\\hat\n",
    "{s}^{\\intercal}\\Phi\\theta\\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "which has a linear programming fomulation\n",
    "\n",
    "\\begin{align*}\n",
    "&  \\min_{m,\\theta}m-\\hat{s}^{\\intercal}\\Phi\\theta\\\\\n",
    "s.t.~ &  m-\\left(  \\Phi\\theta\\right)  _{y}\\geq\\forall y\\in\\mathcal{Y}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-identification\n",
    "\n",
    "Note that the set of $\\theta$ that enter the solution to the problem above is not unique, but is a convex set. Denoting $V$ the value of program, we can look for bounds of $\\theta^{\\intercal}d$ for a chosen direction $d$ by\n",
    "\n",
    "\\begin{align*}\n",
    "& \\min_{\\theta,m}/\\max_{\\theta,m}   \\theta^{\\intercal}d\\\\\n",
    "s.t.~  &  m-\\hat{s}^{\\intercal}X\\theta=V\\\\\n",
    "&  m\\geq\\left(  \\Phi\\theta\\right)_{y}, \\quad \\forall y\\in\\mathcal{Y}%\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Back to the travel mode example of Greene and Hensher (1997). For each individual $i$, we have access to: $y$=travel mode (air, train, bus, car); $T_{iy}$=time taken (observed); $C_{iy}$=generalized cost for passenger (observed); $I_{i}$=income; $y_{j}$=travel mode actually chosen.\n",
    "* Load the data using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(Matrix)\n",
    "library(numDeriv)\n",
    "library(gurobi)\n",
    "\n",
    "thePath = getwd()\n",
    "travelmodedataset = as.matrix(read.csv(paste0(thePath, \"/../data_mec_optim/demand_travelmode/travelmodedata.csv\"), sep = \",\", \n",
    "    header = TRUE))  # loads the data\n",
    "# Convert strings to categorical variables\n",
    "convertmode = Vectorize(function(inputtxt) {\n",
    "    if (inputtxt == \"air\") {\n",
    "        return(1)\n",
    "    }\n",
    "    if (inputtxt == \"train\") {\n",
    "        return(2)\n",
    "    }\n",
    "    if (inputtxt == \"bus\") {\n",
    "        return(3)\n",
    "    }\n",
    "    if (inputtxt == \"car\") {\n",
    "        return(4)\n",
    "    }\n",
    "})\n",
    "\n",
    "convertchoice = function(x) (ifelse(x==\"no\",0,1))\n",
    "\n",
    "travelmodedataset[,2] = convertmode(travelmodedataset[,2])\n",
    "travelmodedataset[,3] = convertchoice(travelmodedataset[,3])\n",
    "nobs = dim(travelmodedataset)[1]\n",
    "nchoices = 4\n",
    "ninds = nobs / nchoices\n",
    "ncols =  dim(travelmodedataset)[2]\n",
    "travelmodedataset = array(as.numeric(travelmodedataset),dim = c(4,ninds,ncols))\n",
    "muhat_i_y = t(travelmodedataset[,,3])\n",
    "muhat_iy = c(muhat_i_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our model is:\n",
    "    + $u_{iy} = U_{y}-T_{iy}(a+bI_{i})-c C_{iy}$\n",
    "* We need to take a normalization, so let's set $U_1 = 0$. \n",
    "* Setup the matrix $\\Phi$ using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Phi_iy_k=cbind( kronecker(sparseMatrix(i = c(2,3,4),j=c(1,2,3)),matrix(1,ninds,1) ), # fixed effect with normalization U_1 = 0\n",
    "              - c(t(travelmodedataset[,,6])), # time\n",
    "              - c(t(travelmodedataset[,,6]*c(travelmodedataset[,,8]))), # time*incime\n",
    "              - c(t(travelmodedataset[,,7]) ) # cost\n",
    ")\n",
    "nbK = dim(Phi_iy_k)[2]\n",
    "\n",
    "mean_k = apply(Phi_iy_k,FUN = mean , MARGIN =  2)\n",
    "std_k = apply(Phi_iy_k,FUN = sd , MARGIN =  2)\n",
    "\n",
    "Phi_iy_k = (Phi_iy_k - matrix(mean_k,nobs,nbK, byrow = T)) / matrix(std_k,nobs,nbK, byrow = T)\n",
    "\n",
    "theta0 = rep(0,nbK)\n",
    "sigma = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, define the log-likelihood function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logLikelihood = function (theta ) {\n",
    "  nbk = length(theta)\n",
    "    \n",
    "  Xtheta = Phi_iy_k %*% theta / sigma\n",
    "    \n",
    "  Xthetamat_iy= matrix(Xtheta,ninds,nchoices)\n",
    "    \n",
    "  max_i = apply(X=Xthetamat_iy,FUN = max,MARGIN = 1)\n",
    "    \n",
    "  expPhi_iy = exp(Xthetamat_iy - matrix(max_i,ninds,nchoices))\n",
    "  d_i = apply(X=expPhi_iy , FUN=sum,MARGIN = 1 )\n",
    "  n_i_k = apply(X= array (Phi_iy_k*c(expPhi_iy),dim = c(ninds,nchoices,nbK) ), FUN=sum,MARGIN = c(1,3) )\n",
    "  thegrad = c(as.matrix(matrix(muhat_iy,1,nchoices*ninds) %*% Phi_iy_k))- apply( X = n_i_k / d_i, FUN = sum, MARGIN=2)\n",
    "  res= sum(Xtheta*muhat_iy)  - sum(max_i) - sigma * sum(log(d_i ))\n",
    "  \n",
    "  thegrad  = - thegrad\n",
    "  res = - res\n",
    "  \n",
    "  attr(res,'gradient') = thegrad\n",
    "  return(res)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The maximization of the log-likelihood is done using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outcome_mle = nlm(f = logLikelihood, p = theta0 , gradtol=1E-8)\n",
    "temp_mle = 1 / outcome_mle$estimate[1]\n",
    "theta_mle = outcome_mle$estimate[-1] * temp_mle\n",
    "temp_mle\n",
    "theta_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We now compute the minimax-regret estimator using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obj = c(c(as.matrix(matrix(muhat_iy,1,nchoices*ninds) %*% Phi_iy_k)),-rep(1,ninds))\n",
    "lengthobj = length(obj)\n",
    "cstMat = cbind( -Phi_iy_k, kronecker(matrix(1,nchoices,1),sparseMatrix(i = 1:ninds , j = 1:ninds,x = 1 ))  )\n",
    "cstMat = rbind(cstMat,c(1,rep(0,lengthobj-1)))\n",
    "nbCstr = dim(cstMat)[1]\n",
    "result = gurobi(list(A = cstMat, obj = obj, modelsense = \"max\", rhs = c(rep(0,nbCstr-1),1), sense =  c(rep(\">\",nbCstr-1),\"=\"),lb=-Inf))\n",
    "theta_lp = result$x[1:nbK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare the MLE and the minmax-regret estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1.0373879252498</li>\n",
       "\t<li>0.971257823301769</li>\n",
       "\t<li>1.01393765532723</li>\n",
       "\t<li>0.185743065764869</li>\n",
       "\t<li>-0.239283563791251</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1.0373879252498\n",
       "\\item 0.971257823301769\n",
       "\\item 1.01393765532723\n",
       "\\item 0.185743065764869\n",
       "\\item -0.239283563791251\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1.0373879252498\n",
       "2. 0.971257823301769\n",
       "3. 1.01393765532723\n",
       "4. 0.185743065764869\n",
       "5. -0.239283563791251\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  1.0373879  0.9712578  1.0139377  0.1857431 -0.2392836"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(theta_mle)\n",
    "print(theta_lp[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the fixed temperature MLE for intermediate temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indMax=100\n",
    "tempMax=temp_mle\n",
    "outcomemat = matrix(0,indMax+1,nbK-1)\n",
    "for (k in 2:(indMax+1))\n",
    "{\n",
    "  thetemp = tempMax * (k-1)/indMax\n",
    "  logLikelihoodFixedTemp = function(subsetoftheta )\n",
    "  {\n",
    "    theres = logLikelihood(c(1/thetemp,subsetoftheta))\n",
    "    attr(theres,'gradient') = attr(theres,'gradient')[-1]\n",
    "    #print(c(theres))\n",
    "    return(theres)\n",
    "  }\n",
    "  outcomeFixedTemp = nlm(f = logLikelihoodFixedTemp, p = theta0[-1] , gradtol=1E-8)\n",
    "  outcomemat[k,] = outcomeFixedTemp$estimate * thetemp\n",
    "}\n",
    "outcomemat[1,] = theta_lp[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The intermediate estimators are\"\n",
      "            [,1]      [,2]      [,3]      [,4]         [,5]\n",
      "  [1,] 1.0373879 0.9712578 1.0139377 0.1857431 -0.239283564\n",
      "  [2,] 1.0380721 0.9737737 1.0171864 0.1853983 -0.253452642\n",
      "  [3,] 1.0337537 0.9723427 1.0165336 0.1855482 -0.251090689\n",
      "  [4,] 1.0277486 0.9715822 1.0137210 0.1886797 -0.249606071\n",
      "  [5,] 1.0214581 0.9707601 1.0096556 0.1938048 -0.247751296\n",
      "  [6,] 1.0150357 0.9694446 1.0050014 0.1999665 -0.244856011\n",
      "  [7,] 1.0085818 0.9676141 1.0002467 0.2065499 -0.240953984\n",
      "  [8,] 1.0021926 0.9653773 0.9956205 0.2132643 -0.236251961\n",
      "  [9,] 0.9959256 0.9628552 0.9911995 0.2199963 -0.230959757\n",
      " [10,] 0.9897980 0.9601467 0.9869953 0.2267022 -0.225247480\n",
      " [11,] 0.9838016 0.9573245 0.9829979 0.2333599 -0.219242680\n",
      " [12,] 0.9779182 0.9544391 0.9791922 0.2399529 -0.213037667\n",
      " [13,] 0.9721284 0.9515247 0.9755620 0.2464675 -0.206697394\n",
      " [14,] 0.9664146 0.9486043 0.9720901 0.2528937 -0.200265683\n",
      " [15,] 0.9607626 0.9456922 0.9687579 0.2592255 -0.193770014\n",
      " [16,] 0.9551610 0.9427971 0.9655455 0.2654609 -0.187225451\n",
      " [17,] 0.9496006 0.9399238 0.9624319 0.2716010 -0.180637962\n",
      " [18,] 0.9440744 0.9370742 0.9593960 0.2776498 -0.174007059\n",
      " [19,] 0.9385767 0.9342485 0.9564174 0.2836124 -0.167328105\n",
      " [20,] 0.9331029 0.9314455 0.9534768 0.2894955 -0.160594001\n",
      " [21,] 0.9276495 0.9286635 0.9505565 0.2953057 -0.153796488\n",
      " [22,] 0.9222137 0.9259006 0.9476404 0.3010501 -0.146927073\n",
      " [23,] 0.9167932 0.9231545 0.9447147 0.3067355 -0.139977658\n",
      " [24,] 0.9113865 0.9204231 0.9417672 0.3123682 -0.132940911\n",
      " [25,] 0.9059920 0.9177043 0.9387877 0.3179545 -0.125810529\n",
      " [26,] 0.9006088 0.9149963 0.9357674 0.3235000 -0.118581312\n",
      " [27,] 0.8952362 0.9122975 0.9326994 0.3290099 -0.111249169\n",
      " [28,] 0.8898734 0.9096063 0.9295780 0.3344889 -0.103811094\n",
      " [29,] 0.8845201 0.9069215 0.9263986 0.3399415 -0.096265048\n",
      " [30,] 0.8791758 0.9042419 0.9231579 0.3453716 -0.088609930\n",
      " [31,] 0.8738404 0.9015666 0.9198531 0.3507827 -0.080845398\n",
      " [32,] 0.8685137 0.8988949 0.9164827 0.3561781 -0.072971808\n",
      " [33,] 0.8631954 0.8962260 0.9130454 0.3615606 -0.064990084\n",
      " [34,] 0.8578855 0.8935593 0.9095408 0.3669329 -0.056901658\n",
      " [35,] 0.8525839 0.8908945 0.9059687 0.3722973 -0.048708324\n",
      " [36,] 0.8472903 0.8882310 0.9023294 0.3776558 -0.040412213\n",
      " [37,] 0.8420049 0.8855687 0.8986236 0.3830103 -0.032015707\n",
      " [38,] 0.8367273 0.8829071 0.8948521 0.3883624 -0.023521370\n",
      " [39,] 0.8314577 0.8802462 0.8910160 0.3937136 -0.014931903\n",
      " [40,] 0.8261958 0.8775858 0.8871165 0.3990652 -0.006250111\n",
      " [41,] 0.8209415 0.8749256 0.8831551 0.4044183  0.002521155\n",
      " [42,] 0.8156948 0.8722656 0.8791331 0.4097739  0.011379016\n",
      " [43,] 0.8104556 0.8696057 0.8750522 0.4151328  0.020320585\n",
      " [44,] 0.8052236 0.8669459 0.8709139 0.4204957  0.029342988\n",
      " [45,] 0.7999989 0.8642860 0.8667199 0.4258634  0.038443453\n",
      " [46,] 0.7947812 0.8616261 0.8624717 0.4312364  0.047619143\n",
      " [47,] 0.7895705 0.8589660 0.8581710 0.4366151  0.056867366\n",
      " [48,] 0.7843666 0.8563058 0.8538194 0.4420000  0.066185473\n",
      " [49,] 0.7791693 0.8536455 0.8494186 0.4473913  0.075570892\n",
      " [50,] 0.7739786 0.8509850 0.8449701 0.4527895  0.085021126\n",
      " [51,] 0.7687942 0.8483244 0.8404755 0.4581946  0.094533763\n",
      " [52,] 0.7636161 0.8456635 0.8359363 0.4636068  0.104106469\n",
      " [53,] 0.7584441 0.8430025 0.8313540 0.4690264  0.113736998\n",
      " [54,] 0.7532781 0.8403413 0.8267301 0.4744533  0.123423184\n",
      " [55,] 0.7481179 0.8376800 0.8220659 0.4798877  0.133162910\n",
      " [56,] 0.7429633 0.8350185 0.8173628 0.4853297  0.142954244\n",
      " [57,] 0.7378143 0.8323567 0.8126221 0.4907792  0.152795226\n",
      " [58,] 0.7326707 0.8296949 0.8078452 0.4962362  0.162684009\n",
      " [59,] 0.7275324 0.8270328 0.8030332 0.5017007  0.172618824\n",
      " [60,] 0.7223992 0.8243706 0.7981873 0.5071726  0.182597975\n",
      " [61,] 0.7172710 0.8217082 0.7933088 0.5126520  0.192619834\n",
      " [62,] 0.7121476 0.8190457 0.7883986 0.5181387  0.202682923\n",
      " [63,] 0.7070290 0.8163830 0.7834579 0.5236327  0.212785606\n",
      " [64,] 0.7019151 0.8137202 0.7784878 0.5291339  0.222926519\n",
      " [65,] 0.6968056 0.8110572 0.7734892 0.5346422  0.233104292\n",
      " [66,] 0.6917005 0.8083941 0.7684630 0.5401575  0.243317611\n",
      " [67,] 0.6865997 0.8057309 0.7634103 0.5456796  0.253565221\n",
      " [68,] 0.6815030 0.8030675 0.7583318 0.5512086  0.263845918\n",
      " [69,] 0.6764104 0.8004040 0.7532284 0.5567442  0.274158548\n",
      " [70,] 0.6713217 0.7977404 0.7481010 0.5622864  0.284502008\n",
      " [71,] 0.6662369 0.7950767 0.7429504 0.5678350  0.294875238\n",
      " [72,] 0.6611557 0.7924128 0.7377772 0.5733900  0.305277225\n",
      " [73,] 0.6560782 0.7897488 0.7325823 0.5789511  0.315706997\n",
      " [74,] 0.6510043 0.7870847 0.7273663 0.5845184  0.326163623\n",
      " [75,] 0.6459337 0.7844205 0.7221300 0.5900916  0.336646209\n",
      " [76,] 0.6408665 0.7817562 0.7168739 0.5956707  0.347153899\n",
      " [77,] 0.6358026 0.7790918 0.7115986 0.6012555  0.357685871\n",
      " [78,] 0.6307419 0.7764273 0.7063049 0.6068459  0.368241338\n",
      " [79,] 0.6256842 0.7737626 0.7009932 0.6124419  0.378819543\n",
      " [80,] 0.6206296 0.7710979 0.6956641 0.6180432  0.389419760\n",
      " [81,] 0.6155779 0.7684331 0.6903182 0.6236498  0.400041292\n",
      " [82,] 0.6105290 0.7657682 0.6849560 0.6292616  0.410683465\n",
      " [83,] 0.6054830 0.7631031 0.6795779 0.6348784  0.421345653\n",
      " [84,] 0.6004396 0.7604380 0.6741845 0.6405001  0.432027190\n",
      " [85,] 0.5953989 0.7577728 0.6687762 0.6461267  0.442727579\n",
      " [86,] 0.5903608 0.7551075 0.6633534 0.6517581  0.453446165\n",
      " [87,] 0.5853251 0.7524421 0.6579166 0.6573941  0.464182427\n",
      " [88,] 0.5802919 0.7497766 0.6524661 0.6630346  0.474935847\n",
      " [89,] 0.5752611 0.7471111 0.6470025 0.6686795  0.485705883\n",
      " [90,] 0.5702326 0.7444454 0.6415260 0.6743288  0.496492066\n",
      " [91,] 0.5652064 0.7417797 0.6360371 0.6799824  0.507293942\n",
      " [92,] 0.5601824 0.7391138 0.6305361 0.6856400  0.518111066\n",
      " [93,] 0.5551606 0.7364479 0.6250233 0.6913017  0.528942971\n",
      " [94,] 0.5501408 0.7337819 0.6194989 0.6969675  0.539789226\n",
      " [95,] 0.5451231 0.7311159 0.6139636 0.7026370  0.550649478\n",
      " [96,] 0.5401074 0.7284497 0.6084174 0.7083104  0.561523291\n",
      " [97,] 0.5350937 0.7257835 0.6028607 0.7139875  0.572410305\n",
      " [98,] 0.5300818 0.7231172 0.5972938 0.7196683  0.583310150\n",
      " [99,] 0.5250718 0.7204508 0.5917169 0.7253526  0.594222480\n",
      "[100,] 0.5200637 0.7177843 0.5861303 0.7310404  0.605146968\n",
      "[101,] 0.5150573 0.7151178 0.5805343 0.7367316  0.616083285\n"
     ]
    }
   ],
   "source": [
    "print('The intermediate estimators are')\n",
    "print(outcomemat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "* Compute the minimax estimator of the simulated logit mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbB=100\n",
    "thetemp = 1\n",
    "#epsilon_biy = array(0, dim=c(nbB,nchoices,ninds))\n",
    "epsilon_biy =  array(digamma(1) - log(-log(runif(nbB*ninds*nchoices))), dim=c(nbB,ninds,nchoices))\n",
    "#Phi_biy_k = kronecker(Phi_iy_k,matrix(1,nbB,1)) + kronecker(c(epsilon_biy),matrix(1,1,nbK))\n",
    "muhat_biy = rep(muhat_i_y,each=nbB)\n",
    "newobj = c(c(as.matrix(matrix(muhat_iy,1,nchoices*ninds) %*% Phi_iy_k)),-rep(1,ninds*nbB)/nbB  )\n",
    "newlengthobj = length(newobj)\n",
    "cstr1 = kronecker(-Phi_iy_k,matrix(1,nbB,1))\n",
    "newcstMat = cbind( kronecker(-Phi_iy_k,matrix(1,nbB,1)) , kronecker(matrix(1,nchoices,1),sparseMatrix(i = 1:(ninds*nbB) , j = 1:(ninds*nbB),x = 1 ))  )\n",
    "newnbCstr = dim(newcstMat)[1]\n",
    "newresult = gurobi(list(A = newcstMat, obj = newobj, modelsense = \"max\", rhs = c(epsilon_biy), sense =  \">\",lb=-Inf), params = list(OutputFlag = 0))\n",
    "\n",
    "newtheta_lp = newresult$x[1:nbK] / newresult$x[1] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare the MLE estimator in the logit model and the minimax regret estimator in the simulated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_mle\n",
    "newtheta_lp[-1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
